---
title: "W271 Group Lab 1"
subtitle: 'Due 4:00pm Pacific Time Monday June 1 2020'
students: 'Katie Mo, Rajiv Nair, David Linnard Wheeler'
output: 
  pdf_document:
  toc: true
  number_sections: true
fontsize: 11pt
geometry: margin=1in
---

## Instructions (Please Read Carefully):

* 20 page limit (strict)

* Do not modify fontsize, margin or line_spacing settings

* One student from each group should submit the lab to their student github repo by the deadline; submission and revisions made after the deadline will not be graded

* Answers should clearly explain your reasoning; do not simply 'output dump' the results of code without explanation 

* Submit two files:
    
    1. A pdf file that details your answers. Include all R code used to produce the answers. Do not suppress the codes in your pdf file
    
    2. The R markdown (Rmd) file used to produce the pdf file
  
    The assignment will not be graded unless **both** files are submitted
      
* Name your files to include all group members names. For example the students' names are Stan Cartman and Kenny Kyle, name your files as follows:

    * `StanCartman_KennyKyle_Lab1.Rmd`
    * `StanCartman_KennyKyle_Lab1.pdf`
            
* Although it sounds obvious, please write your name on page 1 of your pdf and Rmd files

* All answers should include a detailed narrative; make sure that your audience can easily follow the logic of your analysis. All steps used in modelling must be clearly shown and explained

* For statistical methods that we cover in this course, use the R libraries and functions that are covered in this course. If you use libraries and functions for statistical modeling that we have not covered, you must provide an explanation of why such libraries and functions are used and reference the library documentation. For data wrangling and data visualization, you are free to use other libraries, such as dplyr, ggplot2, etc

* For mathematical formulae, type them in your R markdown file. Do not e.g. write them on a piece of paper, snap a photo, and use the image file

* Incorrectly following submission instructions results in deduction of grades

* Students are expected to act with regard to UC Berkeley Academic Integrity.

\newpage

# Investigation of the 1989 Space Shuttle Challenger Accident 

Carefullly read the Dalal et al (1989) paper (Skip Section 5).

# Load libraries
```{r}
library(ggplot2) # for plotting
library(GGally) # for scatterplot matrices
library(gridExtra) # for arranging multiple plots together
library(car) # For Likelihood Ratio Tests on the fly
library(data.table) # to enable creation and coercion of data tables
library(stargazer) # to tabulate regression tables
library(skimr) # for basic EDA
library(Hmisc) # for basic EDA
library(mcprofile) # for profile likelihoods
library(dplyr) # for coercing dataframes and summary statistic
```

# Load data
```{r}
df = read.table("challenger.csv",
                 header=T, sep=",")
```

**Part 1 (25 points)**

Conduct a thorough EDA of the data set. This should include both graphical and tabular analysis as taught in this course. Output-dump (that is, graphs and tables that don't come with explanations) will result in a very low, if not zero, score. Since the report has a page-limit, you will have to be selective when choosing visuals to illustrate your key points, associated with a concise explanation of the visuals. This EDA should begin with an inspection of the given dataset; examination of anomalies, missing values, potential of top and/or bottom code etc.   

# EDA
- What is the structure of the data?
```{r}
# Structure
str(df)
```

```{r}
# This seems like valuable code?
df$O.ring.binary <- factor(ifelse(df$O.ring > 0,
                                  yes = 1,
                                  no = 0))
#df$O.ring <- factor(df$O.ring)
```


- Are there any missing values? How are the data distributed? What are the unique values?
```{r}
# Summary statistics
skim(df)
# What are the unique values
lapply(df, unique)
```

- How are O.Rings related to putative predictors, like temperature?
```{r, fig.width=5, fig.height=3}
ggplot(df, aes(x=Temp, y=O.ring)) +
  geom_jitter(size = 10, alpha = 0.75, height=.3, width=.3) +
  theme_classic() +
  theme(text = element_text(size = 20)) 
```


- And pressure?
```{r, fig.width=5, fig.height=3}
ggplot(df, aes(x=Pressure, y=O.ring)) +
  geom_jitter(size=20, pch=20, alpha=3/4, position=position_jitter(w=0.2, h=0.1))+
  theme_classic() +
  theme(text = element_text(size = 20)) 
```

- How many observations are present for each level of O.ring failure?
```{r}
# Cross tabulation
plot(table(df$O.ring))
```

- At what temperature do 2 O.rings fail?
```{r}
# Cross tabulation
table(df$O.ring, df$Temp)
```


From the book:
• Flight: Flight number
• Temp: Temperature (F) at launch
• Pressure: Combustion pressure (psi)
• O.ring: Number of primary field O-ring failures
• Number: Total number of primary field O-rings (six total, three each for the two booster rockets)

**Part 2 (20 points)** 

Answer the following from Question 4 of Bilder and Loughin Section 2.4 Exercises (page 129):

(a) The authors use logistic regression to estimate the probability an O-ring will fail. In order to use this model, the authors needed to assume that each O-ring is independent for each launch. Discuss why this assumption is necessary and the potential problems with it. Note that a subsequent analysis helped to alleviate the authors’ concerns about independence.

- The authors state that the binomial but not the binary model suffer from the assumption that each O-ring failure is independent. More specifically, the binomial model assumes that each of the six 0-rings fail independently at each temperature, $t$, and pressure, $s$. 
- The independence assumption is necessary for the binomial model becuase it allows us to use the binomial probability mass function.
- Moreover, the assumption is likely violated since the failure of 1 0-ring might expose other 0-rings to damaging conditions and or might exert stress on the remaining 0-rings.


(b) Estimate the logistic regression model using the explanatory variables in a linear form.

```{r}
#create a dataset that assumes that each of the 6 primary field O-rings on a flight is an independent data point

df_ind <- data.frame()

for(i in array(1:nrow(df))){
  n = df['O.ring'][i,]
  one_flight <- data.frame(Flight=array(df['Flight'][i,], 6),
                           Temp=array(df['Temp'][i,], 6),
                           Pressure=array(df['Pressure'][i,], 6),
                           Failure=c(array(1, n),array(0, 6-n))
                           )
  df_ind <- rbind(df_ind, one_flight)
}
```

- Logistic regression
```{r}
# Estimate logistic regression model
model_a <- glm(Failure ~ Temp + Pressure,
               family = binomial(link = logit),
               data = df_ind)
# Summary of coefficients, standard errors and p-values
summary(model_a)

# vs

## Binomial model with weights
# Estimate logistic regression model
model_a.2 <- glm(O.ring/Number ~ Temp + Pressure,
               family = binomial(link = logit),
               weights = Number,
               data = df)
# Summary of coefficients, standard errors and p-values
summary(model_a.2)
```

(c) Perform LRTs to judge the importance of the explanatory variables in the model.

```{r}
Anova(model_a, test='LR')
```


(d) The authors chose to remove Pressure from the model based on the LRTs. Based on your results, discuss why you think this was done. Are there any potential problems with removing this variable?

- `Pressure` is not an important explanatory variable. 
- Above, we tested $H_0: \beta = 0$ vs $H_A:\beta \neq 0$, for `Pressure`.The $-2log(\Lambda)$ = `r Anova(model_a, test='LR')[2,1]` and the *p*-value of $P(\chi^2_1 > $`r Anova(model_a, test='LR')[2,1]`) = `r Anova(model_a, test='LR')[2,3]` > $\alpha$ 0.05. 
- Thus, `Pressure` was probably removed in response to it lack of importance detected by the likelihood ratio test.
- Although it was removed, it *could* be an important covariate to include in the model. 

**Part 3 (35 points)**

Answer the following from Question 5 of Bilder and Loughin Section 2.4 Exercises (page 129-130):

Continuing Exercise 4, consider the simplified model $logit(\pi) = \beta_0 +  \beta_1 Temp$, where $\pi$ is the probability of an O-ring failure. Complete the following:

(a) Estimate the model.

```{r}
model_b <- glm(Failure ~ Temp ,
               family = binomial(link = logit),
               data = df_ind)
summary(model_b)
# Estimate logistic regression model
model_b.2 <- glm(O.ring/Number ~ Temp,
               family = binomial(link = logit),
               weights = Number,
               data = df)
# Summary of coefficients, standard errors and p-values
summary(model_b.2)
```



(b) Construct two plots: (1) $\pi$ vs. Temp and (2) Expected number of failures vs. Temp. Use a temperature range of 31° to 81° on the x-axis even though the minimum temperature in the data set was 53°.

- (1) $\pi$ vs. Temp:
```{r}
plot(df$Temp, df$O.ring/df$Number,
     xlab='Temperature',
     ylab=expression(pi),
     pch=20, cex=2,
     xlim=c(31,81),
     ylim=c(0,1))
# Betas
b0 <- model_b$coefficients[1]
b1 <- model_b$coefficients[2]
# Curve
curve(expr = exp(b0+b1*x)/(1+exp(b0+b1*x)), add=T)
```


- (2) Expected number of failures vs. Temp.


(c) Include the 95% Wald confidence interval bands for $\pi$ on the plot. Why are the bands much wider for lower temperatures than for higher temperatures?

```{r}
t <- seq(31,81,1)
alpha=0.05
model_b_predict <- predict(object=model_b, newdata=data.frame(Temp=t), type='link', se=T)
CI_lower_linear <- model_b_predict$fit + qnorm(p=alpha/2)*model_b_predict$se.fit
CI_lower_pi <- exp(CI_lower_linear)/(1+exp(CI_lower_linear))
CI_higher_linear <- model_b_predict$fit + qnorm(p=1-alpha/2)*model_b_predict$se.fit
CI_higher_pi <- exp(CI_higher_linear)/(1+exp(CI_higher_linear))
```


```{r}
plot(df$Temp, df$O.ring/df$Number,
     xlab='Temperature',
     ylab=expression(pi),
     pch=20, cex=2,
     xlim=c(31,81),
     ylim=c(0,1))
b0 <- model_b$coefficients[1]
b1 <- model_b$coefficients[2]
curve(expr = exp(b0+b1*x)/(1+exp(b0+b1*x)), add=T)
lines(t, CI_lower_pi, lty = 'dashed')
lines(t, CI_higher_pi, lty = 'dashed')
```

- The confidence band is wider when temperatures are lower than`r min(df$Temp)` because there are no observations below `r min(df$Temp)`.

(d) The temperature was 31° at launch for the Challenger in 1986. Estimate the probability of an O-ring failure using this temperature, and compute a corresponding confidence interval. Discuss what assumptions need to be made in order to apply the inference procedures.

- The probability of O-ring failure when temperature is 31°:
```{r}
# Data to exptrapolate
predict.data <- data.frame(Temp = 20)
# Predict surface/link
predict(object = model_b,
        newdata = predict.data,
        type = "link")
# Predict response 
predict(object = model_b, 
        newdata = predict.data,
        type = "response")
# Confidence interval
c(CI_lower_pi[[1]], CI_higher_pi[[1]])
```

- Thus, the probability of O-ring failure is `r predict(object = model_b, newdata = predict.data, type = "response")` with a very wide confidence interval of `r c(CI_lower_pi[[1]], CI_higher_pi[[1]])`.
- The assumptions needed to apply the inference procedures are:
  - 
  -
  

(e) Rather than using Wald or profile LR intervals for the probability of failure, Dalal et al. (1989) use a parametric bootstrap to compute intervals. Their process was to (1) simulate a large number of data sets (n = 23 for each) from the estimated model of  Temp; (2) estimate new models for each data set, say and (3) compute  at a specific temperature of interest. The authors used the 0.05 and 0.95 observed quantiles from the  simulated distribution as their 90% confidence interval limits. Using the parametric bootstrap, compute 90% confidence intervals separately at temperatures of 31° and 72°.27

```{r}
# # This might be helpful: https://cran.r-project.org/web/packages/bcaboot/vignettes/bcaboot.html
# glm_boot <- function(B, glm_model, weights, var = "resp") {
#     pi_hat <- model_b$fitted.values
#     n <- length(pi_hat)
#     y_star <- sapply(seq_len(B), function(i) ifelse(runif(n) <= pi_hat, 1, 0))
#     beta_star <- apply(y_star, 2, function(y) {
#         boot_data <- glm_model$data
#         boot_data$y <- y
#         coef(glm(formula = y ~ ., 
#                  data = boot_data,
#                  weights = weights,
#                  family = "binomial"))
#     })
#     list(theta = coef(glm_model)[var],
#          theta_star = beta_star[var, ],
#          suff_stat = t(y_star) %*% model.matrix(model_b))
# }
# 
# glm_boot_out <- glm_boot(B = 2000, glm_model = model_b, weights = weights)
# glm_bca <- bcapar(t0 = glm_boot_out$theta,
#                   tt = glm_boot_out$theta_star,
#                   bb = glm_boot_out$suff_stat)
```



(f) Determine if a quadratic term is needed in the model for the temperature.

**Part 4 (10 points)**

With the same set of explanatory variables in your final model, estimate a linear regression model. Explain the model results; conduct model diagnostic; and assess the validity of the model assumptions.  Would you use the linear regression model or binary logistic regression in this case?  Explain why.

**Part 5 (10 points)**

Interpret the main result of your final model in terms of both odds and probability of failure. Summarize the final result with respect to the question(s) being asked and key takeaways from the analysis.