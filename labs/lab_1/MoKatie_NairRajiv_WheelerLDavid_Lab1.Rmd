---
title: "W271 Group Lab 1"
subtitle: 'Due 4:00pm Pacific Time Monday June 1 2020'
students: 'Katie Mo, Rajiv Nair, David Linnard Wheeler'
output: 
  pdf_document:
  toc: true
  number_sections: true
fontsize: 11pt
geometry: margin=1in
---

## Instructions (Please Read Carefully):

* 20 page limit (strict)

* Do not modify fontsize, margin or line_spacing settings

* One student from each group should submit the lab to their student github repo by the deadline; submission and revisions made after the deadline will not be graded.

* Answers should clearly explain your reasoning; do not simply 'output dump' the results of code without explanation 

* Submit two files:
    
    1. A pdf file that details your answers. Include all R code used to produce the answers. Do not suppress the codes in your pdf file
    
    2. The R markdown (Rmd) file used to produce the pdf file
  
    The assignment will not be graded unless **both** files are submitted
      
* Name your files to include all group members names. For example the students' names are Stan Cartman and Kenny Kyle, name your files as follows:

    * `StanCartman_KennyKyle_Lab1.Rmd`
    * `StanCartman_KennyKyle_Lab1.pdf`
            
* Although it sounds obvious, please write your name on page 1 of your pdf and Rmd files

* All answers should include a detailed narrative; make sure that your audience can easily follow the logic of your analysis. All steps used in modelling must be clearly shown and explained

* For statistical methods that we cover in this course, use the R libraries and functions that are covered in this course. If you use libraries and functions for statistical modeling that we have not covered, you must provide an explanation of why such libraries and functions are used and reference the library documentation. For data wrangling and data visualization, you are free to use other libraries, such as dplyr, ggplot2, etc

* For mathematical formulae, type them in your R markdown file. Do not e.g. write them on a piece of paper, snap a photo, and use the image file

* Incorrectly following submission instructions results in deduction of grades

* Students are expected to act with regard to UC Berkeley Academic Integrity.

\newpage

# Investigation of the 1989 Space Shuttle Challenger Accident 

Carefullly read the Dalal et al (1989) paper (Skip Section 5).

# Load libraries
```{r warning=FALSE, message=FALSE}
library(ggplot2) # for plotting
library(GGally) # for scatterplot matrices
library(gridExtra) # for arranging multiple plots together
library(car) # For Likelihood Ratio Tests on the fly
library(data.table) # to enable creation and coercion of data tables
library(stargazer) # to tabulate regression tables
library(skimr) # for basic EDA
#library(Hmisc) # for basic EDA
library(mcprofile) # for profile likelihoods
library(dplyr) # for coercing dataframes and summary statistic
library(boot) # framework for bootstrapping
#library(plotly) # For 3d scatterplot
```

# Load data
```{r}
df = read.table("challenger.csv",
                 header=T, sep=",")
```

**Part 1 (25 points)**

Conduct a thorough EDA of the data set. This should include both graphical and tabular analysis as taught in this course. Output-dump (that is, graphs and tables that don't come with explanations) will result in a very low, if not zero, score. Since the report has a page-limit, you will have to be selective when choosing visuals to illustrate your key points, associated with a concise explanation of the visuals. This EDA should begin with an inspection of the given dataset; examination of anomalies, missing values, potential of top and/or bottom code etc.   

# EDA
  To understand the data, their structure, range of values, and patterns, we first inspected the structure of the data:
```{r}
# Structure
str(df)
# Create binary column for O.ring failture
df$O.ringBinary = ifelse(test = df$O.ring > 0,
                         yes = 1, 
                         no = 0)
# Convert O.ringBinary to factor
df$O.ringBinary = as.factor(df$O.ringBinary)
# Sanity check
str(df)
```
  Note that the data are a 23 x 5 dimensional data.frame. Each of the 5 variables are treated as `int` classes. For now, this is OK, however, we may transform O.ring into a binary factor for subsequent analyses.

  Next, we inspected basic summary statistics, missing observations, and histograms of the 5 variables:
```{r}
# Summary statistics
skim(df)
```
  Note that there are no missing observations. Moreover, the putative explanatory variables, `Temp` and `Pressure` are skewed. Likewise, the dependent variable `O.ring` is positively skewed.

  To ensure that the data do not contain anomolous observations, we next inspected the unique values for each variable:
```{r}
# What are the unique values
lapply(df, unique)
```
  The values for each variable seem reasonable.
 
- How many observations are present for each level of O.ring failure?
```{r, fig.width=4, fig.height=2.5}
# Cross tabulation
ggplot(as.data.frame(table(df)),
       aes(x = as.factor(O.ring), y = Freq, colour=O.ring>0)) + 
    geom_bar(stat="identity", width=0.1, alpha =0.7, color = "black", fill= "olivedrab") +
    scale_x_discrete(name = "Number of O-ring failures")+
    scale_y_continuous(name = "Counts")+
   theme_classic() +
   theme(text = element_text(size = 20)) +
   labs(title=expression(~bold('Figure 1:')~'Distribution of O-ring failures'))+
   theme(legend.position = "top") +  
   theme(plot.title = element_text(hjust=0)) + 
   theme(
   axis.title.x = element_text(size = 25),
   axis.text.x = element_text(size = 20),
   axis.title.y = element_text(size = 25),
   axis.text.y = element_text(size = 20),
   plot.title = element_text(size = 25))
``` 


- How are O.Rings related to putative predictors, like temperature?
```{r, fig.width=7, fig.height=3}
ggplot(df, aes(x=Temp, y=as.factor(O.ring), colour=O.ring>0)) +
  geom_jitter(size = 10, alpha = 0.75, stroke=3, height=.05, width=.3) +
  scale_color_manual(labels = c("ignored by NASA","used by NASA"),
                                values = rev(c("black","olivedrab"))) +
  scale_x_continuous(name = "Temperature (°F)")+
  scale_y_discrete(name = "Number of O-ring\n failures") +
  geom_rug(alpha=2/4, size=3, position="jitter") +
  theme_classic() +
  theme(text = element_text(size = 20)) +
  labs(title=expression(~bold('Figure 2:')~'O-ring failure as a function of temperature (°F)'),
       color="Observations:")+
  theme(legend.position = "top") +  
  theme(plot.title = element_text(hjust=0)) + 
  theme(
  axis.title.x = element_text(size = 35),
  axis.text.x = element_text(size = 30),
  axis.title.y = element_text(size = 35),
  axis.text.y = element_text(size = 30),
  plot.title = element_text(size = 35)) 
  
```

- And pressure?
```{r, fig.width=7, fig.height=3}
ggplot(df, aes(x=Pressure, y=as.factor(O.ring), colour=O.ring>0)) +
  geom_jitter(size=20, pch=20, alpha=3/4, stroke=3, position=position_jitter(w=0.2, h=0.1))+
  scale_color_manual(labels = c("ignored by NASA","used by NASA"),
                                values = rev(c("black","olivedrab"))) +
  scale_x_continuous(name = "Pressure")+
  scale_y_discrete(name = "Number of O-ring\n failures")+
    geom_rug(alpha=2/4, size=3, position="jitter") +
  theme_classic() +
  theme(text = element_text(size = 20)) +
  labs(title=expression(~bold('Figure 3:')~' O-ring failure as a function of pressure'),
     color="Observations:")+
  theme(legend.position = "top") +
  theme(plot.title = element_text(hjust=0)) + 
  theme(
  axis.title.x = element_text(size = 35),
  axis.text.x = element_text(size = 30),
  axis.title.y = element_text(size = 35),
  axis.text.y = element_text(size = 30),
  plot.title = element_text(size = 35))
```

- Are O-ring failures related to time/flight number
```{r, fig.width=7, fig.height=3}
ggplot(df, aes(x=Flight, y=as.factor(O.ring), colour=O.ring>0)) +
  geom_jitter(size=20, pch=20, alpha=3/4, stroke=3, position=position_jitter(w=0.2, h=0.1))+
  scale_color_manual(labels = c("ignored by NASA","used by NASA"),
                                values = rev(c("black","olivedrab"))) +
  scale_x_continuous(name = "Flight number")+
  scale_y_discrete(name = "Number of O-ring\n failures")+
  geom_rug(alpha=2/4, size=3, position="jitter") +
  theme_classic() +
  theme(text = element_text(size = 20)) +
  labs(title=expression(~bold('Figure 4:')~' O-ring failure as a function of flight number/time'),
   color="Observations:")+
  theme(legend.position = "top") +
  theme(plot.title = element_text(hjust=0)) + 
  theme(
  axis.title.x = element_text(size = 35),
  axis.text.x = element_text(size = 30),
  axis.title.y = element_text(size = 35),
  axis.text.y = element_text(size = 30),
  plot.title = element_text(size = 35))
```

- How do both `Temp` and `Pressure` affect `O.ring` failure?
```{r}
# Specify levels of O.ring failure
df$O.ring[which(df$O.ring == 0)] <- 0
df$O.ring[which(df$O.ring == 1)] <- 1
df$O.ring[which(df$O.ring == 2)] <- 2
# Define 3 level factor of O.ring failures
df$O.ringFactor <- as.factor(df$O.ring)
# Sanity check
levels(df$O.ringFactor)

# Build 3d scatter plot
fig <- plot_ly(df, x = ~ Temp, y = ~ Pressure, z = ~ O.ringFactor,
               color = ~ O.ringFactor,
               colors = c('olivedrab', 'orange2',"orangered2"))
fig <- fig %>% add_markers()
fig <- fig %>% layout(scene = list(xaxis = list(title = 'Temperature (°F)'),
                     yaxis = list(title = 'Pressure'),
                     zaxis = list(title = 'Number of O-ring failures')))

fig
```


- At what temperature do 2 O.rings fail?
```{r}
# Cross tabulation
table(df$O.ring, df$Temp)
## At what temperatures are there 0,1 and 2 failures
# 0 
sort(df$Temp[df$O.ring == 0])
# 1
sort(df$Temp[df$O.ring == 1])
# 2
sort(df$Temp[df$O.ring == 2])
```


From the book:
• Flight: Flight number
• Temp: Temperature (F) at launch
• Pressure: Combustion pressure (psi)
• O.ring: Number of primary field O-ring failures
• Number: Total number of primary field O-rings (six total, three each for the two booster rockets)

**Part 2 (20 points)** 

Answer the following from Question 4 of Bilder and Loughin Section 2.4 Exercises (page 129):

(a) The authors use logistic regression to estimate the probability an O-ring will fail. In order to use this model, the authors needed to assume that each O-ring is independent for each launch. Discuss why this assumption is necessary and the potential problems with it. Note that a subsequent analysis helped to alleviate the authors’ concerns about independence.

- The authors state that the binomial but not the binary model suffer from the assumption that each O-ring failure is independent. More specifically, the binomial model assumes that each of the six 0-rings fail independently at each temperature, $t$, and pressure, $s$. 

- The independence assumption is necessary for the binomial model becuase it allows us to use the binomial probability mass function.

- Moreover, the assumption is likely violated since the failure of 1 0-ring might expose other 0-rings to damaging conditions and or might exert stress on the remaining 0-rings.


(b) Estimate the logistic regression model using the explanatory variables in a linear form.

```{r}
#create a dataset that assumes that each of the 6 primary field O-rings on a flight is an independent data point
df_ind <- data.frame()

for(i in array(1:nrow(df))){
  n = df['O.ring'][i,]
  one_flight <- data.frame(Flight=array(df['Flight'][i,], 6),
                           Temp=array(df['Temp'][i,], 6),
                           Pressure=array(df['Pressure'][i,], 6),
                           Failure=c(array(1, n),array(0, 6-n))
                           )
  df_ind <- rbind(df_ind, one_flight)
}
```

- Logistic regression
```{r, warning=F}
# Estimate logistic regression model
model_a <- glm(O.ring/Number ~ Temp + Pressure,
               family = binomial(link = logit),
               weights = Number,
               data = df)

# Summary of coefficients, standard errors and p-values
stargazer(model_a,
          type="text")
```

$$ logit\left(\hat{\pi}\right)= 2.520 - 0.098\text{Temp} + 0.008\text{Pressure} $$ 


(c) Perform LRTs to judge the importance of the explanatory variables in the model.

  Below we test: $H_0: \beta = 0$ vs $H_A:\beta \neq 0$, for both `Temp` and `Pressure`:
```{r}
Anova(model_a, test='LR')
```
  As discussed below `Temp` but not `Pressure` is important for the model.

(d) The authors chose to remove Pressure from the model based on the LRTs. Based on your results, discuss why you think this was done. Are there any potential problems with removing this variable?

- `Pressure` is not an important explanatory variable. 
- Above, we tested $H_0: \beta = 0$ vs $H_A:\beta \neq 0$, for `Pressure`.The $-2log(\Lambda)$ = `r Anova(model_a, test='LR')[2,1]` and the *p*-value of $P(\chi^2_1 > $`r Anova(model_a, test='LR')[2,1]`) = `r Anova(model_a, test='LR')[2,3]` > $\alpha$ 0.05. 
- Thus, `Pressure` was probably removed in response to it lack of importance detected by the likelihood ratio test & because it does not contribute much systematic variation to the model (i.e `Pressure` can only take values `r unique(df$Pressure)`). 
- Although it was removed, it *could* be an important covariate to include in the model. For example, recall from **Figure 3** that (i) both instances where 2 `O.ring`s failed occured when `Pressure` was high = $200$ and (ii) `r length(df$Pressure[df$O.ring == 1 & df$Pressure == 200])` of `r length(df$Pressure[df$O.ring == 1])` instances where 1 `O.ring` failed occured when `Pressure` was high = $200$. Similarly, the Dalal *et al* 1989 report (i) very weak evidence of a pressure effect and (ii) overlapping confidence intervals for the expected number of incidents when pressure was 50 and 200. Together these bits of evidence support the hypothesis that `Pressure` could contribute to the mechanism by which `O.rings` fail.
- Several problems could arise by the omission of `Pressure` from the model. These problems are both mechanistic and statistical. The mechanistic problem is outline in Dalal *et al* 1989. `O.ring` function and failure are highly influenced by `Pressure`. Thus, while a simple model without `Pressure` might be *sufficient* for ones purposes, a model with `Pressure` and, if more data existed, an interaction term between `Temp` and `Pressure` might help guide interpretation. The statistical problem(s) of removing `Pressure` could be numerous. For one, if `Pressure` is *correlated* with both the explanatory variable `Temp` and the outcome `O.ring`, removal of `Pressure` could introduce omitted variable bias.

**Part 3 (35 points)**

Answer the following from Question 5 of Bilder and Loughin Section 2.4 Exercises (page 129-130):

Continuing Exercise 4, consider the simplified model $logit(\pi) = \beta_0 +  \beta_1 Temp$, where $\pi$ is the probability of an O-ring failure. Complete the following:

(a) Estimate the model.

```{r warning=FALSE}
# Estimate logistic regression model
model_b <- glm(O.ring/Number ~ Temp,
               family = binomial(link = logit),
               weights = Number,
               data = df)
# Summary of coefficients, standard errors and p-values
stargazer(model_b, type="text")
```

$$logit\left(\hat{\pi}\right) = 5.085 - 0.116 Temp$$

(b) Construct two plots: (1) $\pi$ vs. Temp and (2) Expected number of failures vs. Temp. Use a temperature range of 31° to 81° on the x-axis even though the minimum temperature in the data set was 53°.

- (1) $\pi$ vs. Temp:
```{r}
plot(df$Temp, df$O.ring/df$Number,
     xlab='Temperature (°F)',
     ylab=expression(pi),
     pch=20, cex=3,
     col=ifelse(test=df$O.ring > 0, 
                yes="black",
                no="olivedrab"),
     xlim=c(31,81),
     ylim=c(0,1))
# Figure legend
text(x=75, y=0.9,labels = "Figure 4", cex=0.9)
# Betas
b0 <- model_b$coefficients[1]
b1 <- model_b$coefficients[2]
# Curve
curve(expr = exp(b0+b1*x)/(1+exp(b0+b1*x)), add=T)
```


- (2) Expected number of failures vs. Temp.
```{r}
plot(df$Temp, df$O.ring,
     xlab='Temperature (°F)',
     ylab="Expected number of O-ring failures",
     pch=20, cex=3,
     col=ifelse(test=df$O.ring > 0, 
                yes="black",
                no="olivedrab"),
     xlim=c(31,81),
     ylim=c(0,6))
# Figure legend
text(x=75, y=5.5,labels = "Figure 5", cex=0.9)
# Betas
b0 <- model_b$coefficients[1]
b1 <- model_b$coefficients[2]
# Curve
curve(expr = exp(b0+b1*x)/(1+exp(b0+b1*x))*6, add=T)
```

(c) Include the 95% Wald confidence interval bands for $\pi$ on the plot. Why are the bands much wider for lower temperatures than for higher temperatures?

```{r}
t <- seq(31,81,1)
alpha=0.05
model_b_predict <- predict(object=model_b, newdata=data.frame(Temp=t), type='link', se=T)
CI_lower_linear <- model_b_predict$fit + qnorm(p=alpha/2)*model_b_predict$se.fit
CI_lower_pi <- exp(CI_lower_linear)/(1+exp(CI_lower_linear))
CI_higher_linear <- model_b_predict$fit + qnorm(p=1-alpha/2)*model_b_predict$se.fit
CI_higher_pi <- exp(CI_higher_linear)/(1+exp(CI_higher_linear))
```


```{r}
plot(df$Temp, df$O.ring/df$Number,
     xlab='Temperature (°F)',
     ylab=expression(pi),
     pch=20, cex=2,
     xlim=c(31,81),
     ylim=c(0,1))
# Figure legend
text(x=75, y=0.9,labels = "Figure 6", cex=0.9)
# Betas
b0 <- model_b$coefficients[1]
b1 <- model_b$coefficients[2]
curve(expr = exp(b0+b1*x)/(1+exp(b0+b1*x)), add=T)
lines(t, CI_lower_pi, lty = 'dashed')
lines(t, CI_higher_pi, lty = 'dashed')
```

- The confidence band is wider when temperatures are lower than`r min(df$Temp)` because there are no observations below `r min(df$Temp)`.

(d) The temperature was 31° at launch for the Challenger in 1986. Estimate the probability of an O-ring failure using this temperature, and compute a corresponding confidence interval. Discuss what assumptions need to be made in order to apply the inference procedures.

- The probability of O-ring failure when temperature is 31°:
```{r}

# Data to extrapolate
predict.data <- data.frame(Temp = 31)

# Predict surface/link
predict.linear <- predict(object = model_b,
                          newdata = predict.data,
                          type = "link")

# Predict response 
predict.pi <- predict(object = model_b, 
                      newdata = predict.data,
                      type = "response")

# Confidence interval for 31deg was computed above in part (c)
data.frame(estimate=predict.pi, lower=CI_lower_pi[[1]], upper=CI_higher_pi[[1]])
```

```{r}
#THIS CAN BE DELETED
# Data to extrapolate
predict.data <- data.frame(Temp = 72)

# Predict surface/link
predict.linear <- predict(object = model_b,
                          newdata = predict.data,
                          type = "link",
                          se=T)

# Predict response 
predict.pi <- predict(object = model_b, 
                      newdata = predict.data,
                      type = "response")
predict.pi
#pi_hat <- exp(predict.linear$fit)/(1+exp(predict.linear$fit))
alpha=0.1
CI_linear <- predict.linear$fit + qnorm(p=c(alpha/2, 1-alpha/2))*predict.linear$se.fit
exp(CI_linear)/(1+exp(CI_linear))

```

- Thus, the probability of O-ring failure at 31° is `r predict.pi` with a very wide confidence interval of `r c(CI_lower_pi[[1]], CI_higher_pi[[1]])`.
- The assumptions needed to apply the inference procedures are:
  - We are assuming a linear relationship between the log odds of failure of an O-ring and Temperature.
  - Since we are using the binomial model, we are assuming that the observations are independent. i.e. The failure of one O-ring does not increase the odds of the other to fail.
  

(e) Rather than using Wald or profile LR intervals for the probability of failure, Dalal et al. (1989) use a parametric bootstrap to compute intervals. Their process was to (1) simulate a large number of data sets (n = 23 for each) from the estimated model of  Temp; (2) estimate new models for each data set, say and (3) compute  at a specific temperature of interest. The authors used the 0.05 and 0.95 observed quantiles from the  simulated distribution as their 90% confidence interval limits. Using the parametric bootstrap, compute 90% confidence intervals separately at temperatures of 31° and 72°.27. At the end, the CI will be the 0.05 and 0.95 quantile of the accumumated results.


For parametric bootstrap, we do the following steps.
1. Create new column o.ring2 and initialize to o.ring. This column will be updated in each iteration.
2. Calculate Z the linear outcome using $Z = \beta_0+\beta_{Temp}*Temp$ (vector of size 23)
3. Calculate $\hat\pi = \cfrac{e^Z}{ 1+e^Z}$ (vector of size 23)
4. Do the following steps in a loop (1..1000):
* sample 23 points $d$ from original dataset with replacement
* subset $\pi$ values corresponding to the random subset generated above
* Generate vector of size 23 of binomials 0-6 $O.ring2$ that represent the outcomes, using the built in rbinom function by passing in the pi's generated in prior step.
* Fit the binomial logistic regression model with new datapoints $d$ and outcomes $O.ring2$ 
* run predictions for 31 and 72 degrees and save results
5. select the 5% and 95% quantiles from the predictions accumulated in the loop above and report


```{r warning=FALSE}
# Use pi estimated from the model
z = model_b$coefficients["(Intercept)"] + model_b$coefficients["Temp"] * df$Temp
pi = exp(z)/(1+exp(z))

# Save the pi array to the dataframe
df$O.ring.pi <- pi

# Dataframe to populate with results
results <- data.frame(pred.31 = numeric(), pred.72 = numeric())

for (s in 1:1000){
  
  I.sample <- sample(x = 1:nrow(df),
                     size = 23,
                     replace = TRUE)
  # Populate d with samples
  d <- df[I.sample,]  
  # subset the probabilities tied to the sample
  pi.b <- df$O.ring.pi[I.sample]

  # Simulate outcomes using rbinom
  O.ring2 <- rbinom(n=23, # sample size
                size=6, # number of trials
                prob=pi.b) # probability
  d <- data.frame(d, O.ring2)
  
  # Estimate model with rbinom bootstrap outcomes
  mod <- glm(O.ring2/Number ~ Temp,
               family = binomial(link = logit),
               weights = Number,
               data = d)
  
  # Estimate confidence interval for temp = 31
  temp.31.data <- data.frame(Temp=31)
  temp.31 <- predict(object = mod, newdata = temp.31.data,
                      type = "response")
  # Estimate confidence interval for temp = 72
  temp.72.data <- data.frame(Temp=72)
  temp.72 <- predict(object = mod, newdata = temp.72.data,
                      type = "response")
  results <- results %>% add_row(pred.31 = temp.31, pred.72 = temp.72)  
}

paste("For 31 degrees CI =")
quantile(results[,1], probs=c(0.05, 0.95))
paste("For 72 degrees CI =")
quantile(results[,2], probs=c(0.05, 0.95))

# Plot histograms of confidence intervals
par(mfrow=c(2,1))
# Histogram for temp = 31
hist(results[,1],
     breaks=50,
     col="lightskyblue1",
     main="Probability of Failure for temperature (°F)= 31")

# Histogram for temp = 72
hist(results[,2],
     breaks=50,
     col="orange",
     main="Probability of Failure for temperature (°F)= 72")
```


(f) Determine if a quadratic term is needed in the model for the temperature.

- Add quadratic temperature term
```{r warning=FALSE}
# Estimate logistic regression model
model_3f <- glm(O.ring/Number ~ Temp + I(Temp^2),
               family = binomial(link = logit),
               weights = Number,
               data = df)
# Summary of coefficients, standard errors and p-values
stargazer(model_3f, type="text")

```

$$ logit\left(\hat{\pi}\right)=22.126 -0.651Temp + 0.004Temp^2 $$ 

- Conduct LRT:
```{r}
# Compare models with LRT
anova(model_b, model_3f,
      test="Chisq")
```

- No, there is no need for a quadratic term. 
- We tested $H_0: \beta_2{\text{temp}^2} = 0$ vs $H_A: \beta_2{\text{temp}^2} \neq 0$.
- For `temp^2`, $-2log(\Lambda)$ = `r anova(model_b.2, model_3f.2, test="Chisq")[2,4]` and the *p*-value of $P(\chi^2_1 > $`r anova(model_b.2, model_3f.2, test="Chisq")[2,4]`) = `r anova(model_b.2, model_3f.2, test="Chisq")[2,5]` > $\alpha = 0.05$. Thus, `temp^2` is not important, when we hold `temp` constant.

**Part 4 (10 points)**

With the same set of explanatory variables in your final model, estimate a linear regression model. Explain the model results; conduct model diagnostic; and assess the validity of the model assumptions.  Would you use the linear regression model or binary logistic regression in this case?  Explain why.

- Estimation of linear regression model:
```{r warning=FALSE}
# Estimate model
model_4 = lm(formula = O.ring/Number ~ Temp,
             data = df, weights = Number)
# Summary
stargazer(model_4, type="text")
```

$$ \cfrac{O.ring}{Number} = 0.616 - 0.008Temp$$


- Explanation of the model results:


- Model diagnostics:
```{r}
# diagnotics
plot(model_4, which=1,
     pch=20, cex=2)
# Figure legend
text(x=0.18, y=0.3,labels = "Figure 7", cex=0.9)
# diagnotics
plot(model_4, which=2,
     pch=20, cex=2)
# Figure legend
text(x=1.5, y=3.2,labels = "Figure 8", cex=0.9)
```

- Assess the validity of the model assumptions: 
  - **Assumption MLR.1 (Linear in Parameters):**
    - The model is linear in its parameters.The $\hat{\beta} s$ are expressed as additive/linear functions of the response: O-ring failure.
  - **Assumption MLR.2 (Random Sampling):**
    - It seems doubtful that these O-rings were sampled randomly, or even *sampled* at all- **all** O-rings were likely collected from `r max(df$Flight)` flights. Further survival bias could be introduced by the inability of NASA to collect all O-rings from flights that malfunctioned.
  - **Assumption MLR.3 (No Perfect Collinearity):**
    - ince there is only one explanatory variable, it is not perfectly collinear with any other explanatory variable.
  - **Assumption MLR.4 (Zero Conditional Mean):**
    - The zero-conditional mean assumption,$E(\mu|x_1, x_2, x_3,...,x_k)$, is not satisfied, sensu stricto. The residual versus fitted values plot: `r plot(model_4, which=1, pch=19, cex=1, cex.lab=1, cex.main=4, lwd=2)` documents the extent to which this model violates this assumption. If the zero mean assumption were satisfied, then we should expect to see a horizontal red line, centered at zero. In contrast we see that our estimates of $\hat{\mu}$, the residuals, deviate from this expectation dramatically. 
  - **Assumption MLR.5 (Homoskedasticity):**
    - Homoskedasticity does not appear to be satisfied. Evidence for heteroskedasticity is again provided by the residual versus fitted values plot: `r plot(model_4, which=1, pch=19, cex=1, cex.lab=1, cex.main=4, lwd=2)`. If variances were homoskedastic we should see uniform horizontal bands/scatters of points across these plots. However, note that we see non-constant variability in the residuals across the range of fitted values.
  - **Assumption MLR.6 (Normality):**
    - The assumption of normality is not satisfied. Evidence of this assumption is presented in the qq-plot:`r plot(model_4, which=2, pch=19, cex=1, cex.lab=1, cex.main=4, lwd=2)` & a histogram of the response variable: `r hist(df$O.ring, col="grey")`.
  - **Lastly, this model formulation possesses another shortcomings**:
    - The probabilities are linearly related to the predictors for all of their possible values.

- We select the binary logistic regression model because:
  - the response, O-ring failure is either binary (e.g. 0 or 1), binomial (e.g. a proportion from 0 to 1), or counts (e.g. 0,1,2,3,...,n) which can ultimately be expressed as a binary or binomial variable.
  -

- Explain why:

**Part 5 (10 points)**

Interpret the main result of your final model in terms of both odds and probability of failure. Summarize the final result with respect to the question(s) being asked and key takeaways from the analysis.

Our final model model is the binomial logistic regression model $$logit\left(\hat\pi\right) = 5.085 - 0.116 Temp$$

This represents the log odds of the fraction of O-rings that fail. To calculate the odds, we exponentiate the coefficients as below. 


```{r}
exp(model_b$coefficients)

1/exp(model_b$coefficients)
```


Here we see that a unit decrease in temperature increases the odds of failure increases by 1.12 times.

In terms of probability of failure, as is seen in the chart of probability of failure vs temperature in part 3c, we clearly see an increase in the probability of failure with lower temperature. For temperature going from 50F, 40F to 30F, we see corresponding probability of failure $\pi$ values of  0.33, 0.61 to 0.83, which clearly shows that lower temperatures lead to a higher probability of failure.

