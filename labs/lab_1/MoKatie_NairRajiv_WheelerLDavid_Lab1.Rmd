---
title: "W271 Group Lab 1"
subtitle: 'Due 4:00pm Pacific Time Monday June 1 2020'
students: 'Katie Mo, Rajiv Nair, David Linnard Wheeler'
output: 
  pdf_document:
  toc: true
  number_sections: true
fontsize: 11pt
geometry: margin=1in
---

## Instructions (Please Read Carefully):

* 20 page limit (strict)

* Do not modify fontsize, margin or line_spacing settings

* One student from each group should submit the lab to their student github repo by the deadline; submission and revisions made after the deadline will not be graded

* Answers should clearly explain your reasoning; do not simply 'output dump' the results of code without explanation 

* Submit two files:
    
    1. A pdf file that details your answers. Include all R code used to produce the answers. Do not suppress the codes in your pdf file
    
    2. The R markdown (Rmd) file used to produce the pdf file
  
    The assignment will not be graded unless **both** files are submitted
      
* Name your files to include all group members names. For example the students' names are Stan Cartman and Kenny Kyle, name your files as follows:

    * `StanCartman_KennyKyle_Lab1.Rmd`
    * `StanCartman_KennyKyle_Lab1.pdf`
            
* Although it sounds obvious, please write your name on page 1 of your pdf and Rmd files

* All answers should include a detailed narrative; make sure that your audience can easily follow the logic of your analysis. All steps used in modelling must be clearly shown and explained

* For statistical methods that we cover in this course, use the R libraries and functions that are covered in this course. If you use libraries and functions for statistical modeling that we have not covered, you must provide an explanation of why such libraries and functions are used and reference the library documentation. For data wrangling and data visualization, you are free to use other libraries, such as dplyr, ggplot2, etc

* For mathematical formulae, type them in your R markdown file. Do not e.g. write them on a piece of paper, snap a photo, and use the image file

* Incorrectly following submission instructions results in deduction of grades

* Students are expected to act with regard to UC Berkeley Academic Integrity.

\newpage

# Investigation of the 1989 Space Shuttle Challenger Accident 

Carefullly read the Dalal et al (1989) paper (Skip Section 5).

# Load libraries
```{r warning=FALSE, message=FALSE}
library(ggplot2) # for plotting
library(GGally) # for scatterplot matrices
library(gridExtra) # for arranging multiple plots together
library(car) # For Likelihood Ratio Tests on the fly
library(data.table) # to enable creation and coercion of data tables
library(stargazer) # to tabulate regression tables
library(skimr) # for basic EDA
library(Hmisc) # for basic EDA
library(mcprofile) # for profile likelihoods
library(dplyr) # for coercing dataframes and summary statistic
library(boot) # framework for bootstrapping

```

# Load data
```{r}
df = read.table("challenger.csv",
                 header=T, sep=",")
```

**Part 1 (25 points)**

Conduct a thorough EDA of the data set. This should include both graphical and tabular analysis as taught in this course. Output-dump (that is, graphs and tables that don't come with explanations) will result in a very low, if not zero, score. Since the report has a page-limit, you will have to be selective when choosing visuals to illustrate your key points, associated with a concise explanation of the visuals. This EDA should begin with an inspection of the given dataset; examination of anomalies, missing values, potential of top and/or bottom code etc.   

# EDA
  To understand the data, their structure, range of values, and patterns, we first inspected the structure of the data:
```{r}
# Structure
str(df)
```
  Note that the data are a 23 x 5 dimensional data.frame. Each of the 5 variables are treated as `int` classes. For now, this is OK, however, we may transform O.ring into a binary factor for subsequent analyses.

  Next, we inspected basic summary statistics, missing observations, and histograms of the 5 variables:
```{r}
# Summary statistics
skim(df)
```
  Note that there are no missing observations. Moreover, the putative explanatory variables, `Temp` and `Pressure` are skewed. Likewise, the dependent variable `O.ring` is positively skewed.

  To ensure that the data do not contain anomolous observations, we next inspected the unique values for each variable:
```{r}
# What are the unique values
lapply(df, unique)
```
  The values for each variable seem reasonable.
  
- How are O.Rings related to putative predictors, like temperature?
```{r, fig.width=7, fig.height=3}
ggplot(df, aes(x=Temp, y=O.ring, colour= O.ring>0)) +
  geom_jitter(size = 10, alpha = 0.75, height=.1, width=.3) +
  scale_color_hue(l=5, c=15) +
  theme_classic() +
  theme(text = element_text(size = 20)) +
  geom_text(x=80, y=1.9, size=15, label="Figure 1") +
  theme(legend.position = "none")
```


- And pressure?
```{r, fig.width=7, fig.height=3}
ggplot(df, aes(x=Pressure, y=O.ring)) +
  geom_jitter(size=20, pch=20, alpha=3/4, position=position_jitter(w=0.2, h=0.1))+
  theme_classic() +
  theme(text = element_text(size = 20)) +
  geom_text(x=190, y=1.9, size=15, label="Figure 2")
```

- How many observations are present for each level of O.ring failure?
```{r}
# Cross tabulation
plot(table(df$O.ring),
     xlab= "Number of O-ring failures",
     ylab= "Counts",
     lwd=10)
text(x=1.9, y=14,labels = "Figure 3", cex=0.9)
```

- At what temperature do 2 O.rings fail?
```{r}
# Cross tabulation
table(df$O.ring, df$Temp)
## At what temperatures are there 0,1 and 2 failures
# 0 
sort(df$Temp[df$O.ring == 0])
# 1
sort(df$Temp[df$O.ring == 1])
# 2
sort(df$Temp[df$O.ring == 2])
```


From the book:
• Flight: Flight number
• Temp: Temperature (F) at launch
• Pressure: Combustion pressure (psi)
• O.ring: Number of primary field O-ring failures
• Number: Total number of primary field O-rings (six total, three each for the two booster rockets)

**Part 2 (20 points)** 

Answer the following from Question 4 of Bilder and Loughin Section 2.4 Exercises (page 129):

(a) The authors use logistic regression to estimate the probability an O-ring will fail. In order to use this model, the authors needed to assume that each O-ring is independent for each launch. Discuss why this assumption is necessary and the potential problems with it. Note that a subsequent analysis helped to alleviate the authors’ concerns about independence.

- The authors state that the binomial but not the binary model suffer from the assumption that each O-ring failure is independent. More specifically, the binomial model assumes that each of the six 0-rings fail independently at each temperature, $t$, and pressure, $s$. 

- The independence assumption is necessary for the binomial model becuase it allows us to use the binomial probability mass function.

- Moreover, the assumption is likely violated since the failure of 1 0-ring might expose other 0-rings to damaging conditions and or might exert stress on the remaining 0-rings.


(b) Estimate the logistic regression model using the explanatory variables in a linear form.

```{r}
#create a dataset that assumes that each of the 6 primary field O-rings on a flight is an independent data point

df_ind <- data.frame()

for(i in array(1:nrow(df))){
  n = df['O.ring'][i,]
  one_flight <- data.frame(Flight=array(df['Flight'][i,], 6),
                           Temp=array(df['Temp'][i,], 6),
                           Pressure=array(df['Pressure'][i,], 6),
                           Failure=c(array(1, n),array(0, 6-n))
                           )
  df_ind <- rbind(df_ind, one_flight)
}
```

- Logistic regression
```{r}
# # Estimate logistic regression model
# model_a <- glm(Failure ~ Temp + Pressure,
#                family = binomial(link = logit),
#                data = df_ind)
# # Summary of coefficients, standard errors and p-values
# summary(model_a)
# 
# # vs
## Binomial model with weights
# Estimate logistic regression model
model_a <- glm(O.ring/Number ~ Temp + Pressure,
               family = binomial(link = logit),
               weights = Number,
               data = df)

# Summary of coefficients, standard errors and p-values
summary(model_a)
```

(c) Perform LRTs to judge the importance of the explanatory variables in the model.

  Below we test: $H_0: \beta = 0$ vs $H_A:\beta \neq 0$, for both `Temp` and `Pressure`:
```{r}
Anova(model_a, test='LR')
```
  As discussed below `Temp` but not `Pressure` is important for the model.

(d) The authors chose to remove Pressure from the model based on the LRTs. Based on your results, discuss why you think this was done. Are there any potential problems with removing this variable?

- `Pressure` is not an important explanatory variable. 
- Above, we tested $H_0: \beta = 0$ vs $H_A:\beta \neq 0$, for `Pressure`.The $-2log(\Lambda)$ = `r Anova(model_a, test='LR')[2,1]` and the *p*-value of $P(\chi^2_1 > $`r Anova(model_a, test='LR')[2,1]`) = `r Anova(model_a, test='LR')[2,3]` > $\alpha$ 0.05. 
- Thus, `Pressure` was probably removed in response to it lack of importance detected by the likelihood ratio test & because it does not contribute much systematic variation to the model (i.e `Pressure` can only take values `r unique(df$Pressure)`). 
- Although it was removed, it *could* be an important covariate to include in the model. For example, recall from **Figure 3** that (i) both instances where 2 `O.ring`s failed occured when `Pressure` was high = $200$ and (ii) `r length(df$Pressure[df$O.ring == 1 & df$Pressure == 200])` of `r length(df$Pressure[df$O.ring == 1])` instances where 1 `O.ring` failed occured when `Pressure` was high = $200$. Thus, it seems like `Pressure` could contribute to the mechanism by which `O.rings` fail.
- **Problems that might arise from the removal of this variable include- omitted variable?**

**Part 3 (35 points)**

Answer the following from Question 5 of Bilder and Loughin Section 2.4 Exercises (page 129-130):

Continuing Exercise 4, consider the simplified model $logit(\pi) = \beta_0 +  \beta_1 Temp$, where $\pi$ is the probability of an O-ring failure. Complete the following:

(a) Estimate the model.

```{r warning=FALSE}
# Estimate logistic regression model
model_b <- glm(O.ring/Number ~ Temp,
               family = binomial(link = logit),
               weights = Number,
               data = df)
# Summary of coefficients, standard errors and p-values
stargazer(model_b, type="text")
```

$$logit\left(\cfrac{O.ring}{Number}\right) = 5.085 - 0.116 Temp$$

(b) Construct two plots: (1) $\pi$ vs. Temp and (2) Expected number of failures vs. Temp. Use a temperature range of 31° to 81° on the x-axis even though the minimum temperature in the data set was 53°.

- (1) $\pi$ vs. Temp:
```{r}
plot(df$Temp, df$O.ring/df$Number,
     xlab='Temperature',
     ylab=expression(pi),
     pch=20, cex=2,
     xlim=c(31,81),
     ylim=c(0,1))
# Figure legend
text(x=75, y=0.9,labels = "Figure 4", cex=0.9)
# Betas
b0 <- model_b$coefficients[1]
b1 <- model_b$coefficients[2]
# Curve
curve(expr = exp(b0+b1*x)/(1+exp(b0+b1*x)), add=T)
```


- (2) Expected number of failures vs. Temp.
```{r}
plot(df$Temp, df$O.ring,
     xlab='Temperature',
     ylab="Expected number of O-ring failures",
     pch=20, cex=2,
     col=ifelse(test=df$O.ring > 0, 
                yes="black",
                no="grey45"),
     xlim=c(31,81),
     ylim=c(0,6))
# Figure legend
text(x=75, y=5.5,labels = "Figure 5", cex=0.9)
# Betas
b0 <- model_b$coefficients[1]
b1 <- model_b$coefficients[2]
# Curve
curve(expr = exp(b0+b1*x)/(1+exp(b0+b1*x))*6, add=T)
```

(c) Include the 95% Wald confidence interval bands for $\pi$ on the plot. Why are the bands much wider for lower temperatures than for higher temperatures?

```{r}
t <- seq(31,81,1)
alpha=0.05
model_b_predict <- predict(object=model_b, newdata=data.frame(Temp=t), type='link', se=T)
CI_lower_linear <- model_b_predict$fit + qnorm(p=alpha/2)*model_b_predict$se.fit
CI_lower_pi <- exp(CI_lower_linear)/(1+exp(CI_lower_linear))
CI_higher_linear <- model_b_predict$fit + qnorm(p=1-alpha/2)*model_b_predict$se.fit
CI_higher_pi <- exp(CI_higher_linear)/(1+exp(CI_higher_linear))
```


```{r}
plot(df$Temp, df$O.ring/df$Number,
     xlab='Temperature',
     ylab=expression(pi),
     pch=20, cex=2,
     xlim=c(31,81),
     ylim=c(0,1))
# Figure legend
text(x=75, y=0.9,labels = "Figure 6", cex=0.9)
# Betas
b0 <- model_b$coefficients[1]
b1 <- model_b$coefficients[2]
curve(expr = exp(b0+b1*x)/(1+exp(b0+b1*x)), add=T)
lines(t, CI_lower_pi, lty = 'dashed')
lines(t, CI_higher_pi, lty = 'dashed')
```

- The confidence band is wider when temperatures are lower than`r min(df$Temp)` because there are no observations below `r min(df$Temp)`.

(d) The temperature was 31° at launch for the Challenger in 1986. Estimate the probability of an O-ring failure using this temperature, and compute a corresponding confidence interval. Discuss what assumptions need to be made in order to apply the inference procedures.

- The probability of O-ring failure when temperature is 31°:
```{r}
# Data to exptrapolate
predict.data <- data.frame(Temp = 20)
# Predict surface/link
predict(object = model_b,
        newdata = predict.data,
        type = "link")
# Predict response 
predict(object = model_b, 
        newdata = predict.data,
        type = "response")
# Confidence interval
c(CI_lower_pi[[1]], CI_higher_pi[[1]])
```

- Thus, the probability of O-ring failure is `r predict(object = model_b, newdata = predict.data, type = "response")` with a very wide confidence interval of `r c(CI_lower_pi[[1]], CI_higher_pi[[1]])`.
- The assumptions needed to apply the inference procedures are:
  - We are assuming a linear relationship between the log odds of failure of an o-ring and Temperature.
  

(e) Rather than using Wald or profile LR intervals for the probability of failure, Dalal et al. (1989) use a parametric bootstrap to compute intervals. Their process was to (1) simulate a large number of data sets (n = 23 for each) from the estimated model of  Temp; (2) estimate new models for each data set, say and (3) compute  at a specific temperature of interest. The authors used the 0.05 and 0.95 observed quantiles from the  simulated distribution as their 90% confidence interval limits. Using the parametric bootstrap, compute 90% confidence intervals separately at temperatures of 31° and 72°.27

```{r warning=FALSE}
# This might be helpful: https://cran.r-project.org/web/packages/bcaboot/vignettes/bcaboot.html
# http://www-math.mit.edu/~dav/05.dir/class25-slides-all.pdf 


# Do not change original # of failures
df[, "O.ring2"] = df$O.ring

results <- data.frame(pred.31 = numeric(), pred.72 = numeric())
for (s in 1:10){
  I.sample <- sample(1:nrow(df), 23,replace=TRUE)
  d <- df[I.sample,]  

  mod <- glm(O.ring2/Number ~ Temp,
               family = binomial(link = logit),
               weights = Number,
               data = d)
  
  z = mod$coefficients["(Intercept)"] + mod$coefficients["Temp"] * d$Temp

  pi = exp(z)/(1+exp(z))
  pi.O.ring <- rbinom(23, 6, prob=pi)
  
  #update the failures generated from model
  df[I.sample,"O.ring2"] <- pi.O.ring
  temp.31.data <- data.frame(Temp=31)
  temp.31 <- predict(object = mod, newdata = temp.31.data,
                      type = "response")

  temp.72.data <- data.frame(Temp=72)
  temp.72 <- predict(object = mod, newdata = temp.72.data,
                      type = "response")
  results <- results %>% add_row(pred.31 = temp.31, pred.72 = temp.72)  
}


paste("For 31 degrees CI =")
quantile(results[,1], probs=c(0.05, 0.95))
paste("For 72 degrees CI =")
quantile(results[,2], probs=c(0.05, 0.95))

```





(f) Determine if a quadratic term is needed in the model for the temperature.

- Add quadratic temperature term
```{r warning=FALSE}
# Estimate logistic regression model
model_3f <- glm(O.ring/Number ~ Temp + I(Temp^2),
               family = binomial(link = logit),
               weights = Number,
               data = df)
# Summary of coefficients, standard errors and p-values
stargazer(model_3f, type="text")

```

$$ logit\left(\cfrac{O.ring}{Number}\right)=22.126 -0.651Temp + 0.004Temp^2 $$ 

- Conduct LRT:
```{r}
# Compare models with LRT
anova(model_b, model_3f,
      test="Chisq")
```

- No, there is no need for a quadratic term. 
- We tested $H_0: \beta_2{\text{temp}^2} = 0$ vs $H_A: \beta_2{\text{temp}^2} \neq 0$.
- For `temp^2`, $-2log(\Lambda)$ = `r anova(model_b.2, model_3f.2, test="Chisq")[2,4]` and the *p*-value of $P(\chi^2_1 > $`r anova(model_b.2, model_3f.2, test="Chisq")[2,4]`) = `r anova(model_b.2, model_3f.2, test="Chisq")[2,5]` > $\alpha = 0.05$. Thus, `temp^2` is not important, when we hold `temp` constant.

**Part 4 (10 points)**

With the same set of explanatory variables in your final model, estimate a linear regression model. Explain the model results; conduct model diagnostic; and assess the validity of the model assumptions.  Would you use the linear regression model or binary logistic regression in this case?  Explain why.

- Estimation of linear regression model:
```{r warning=FALSE}
# Estimate model
model_4 = lm(formula = O.ring/Number ~ Temp,
             data = df, weights = Number)
# Summary
stargazer(model_4, type="text")
```

$$ \cfrac{O.ring}{Number} = 0.616 - 0.008Temp$$


- Explanation of the model results:


- Model diagnostics:
```{r}
# diagnotics
plot(model_4, which=1,
     pch=20, cex=2)
# Figure legend
text(x=0.18, y=0.3,labels = "Figure 7", cex=0.9)
# diagnotics
plot(model_4, which=2,
     pch=20, cex=2)
# Figure legend
text(x=1.5, y=3.2,labels = "Figure 8", cex=0.9)
```

- Assess the validity of the model assumptions: 
  - **Assumption MLR.1 (Linear in Parameters):**
    - The model is linear in its parameters.The $\hat{\beta} s$ are expressed as additive/linear functions of the response: O-ring failure.
  - **Assumption MLR.2 (Random Sampling):**
    - It seems doubtful that these O-rings were sampled randomly, or even *sampled* at all- **all** O-rings were likely collected from `r max(df$Flight)` flights. Further survival bias could be introduced by the inability of NASA to collect all O-rings from flights that malfunctioned.
  - **Assumption MLR.3 (No Perfect Collinearity):**
    - ince there is only one explanatory variable, it is not perfectly collinear with any other explanatory variable.
  - **Assumption MLR.4 (Zero Conditional Mean):**
    - The zero-conditional mean assumption,$E(\mu|x_1, x_2, x_3,...,x_k)$, is not satisfied, sensu stricto. The residual versus fitted values plot: `r plot(model_4, which=1, pch=19, cex=1, cex.lab=1, cex.main=4, lwd=2)` documents the extent to which this model violates this assumption. If the zero mean assumption were satisfied, then we should expect to see a horizontal red line, centered at zero. In contrast we see that our estimates of $\hat{\mu}$, the residuals, deviate from this expectation dramatically. 
  - **Assumption MLR.5 (Homoskedasticity):**
    - Homoskedasticity does not appear to be satisfied. Evidence for heteroskedasticity is again provided by the residual versus fitted values plot: `r plot(model_4, which=1, pch=19, cex=1, cex.lab=1, cex.main=4, lwd=2)`. If variances were homoskedastic we should see uniform horizontal bands/scatters of points across these plots. However, note that we see non-constant variability in the residuals across the range of fitted values.
  - **Assumption MLR.6 (Normality):**
    - The assumption of normality is not satisfied. Evidence of this assumption is presented in the qq-plot:`r plot(model_4, which=2, pch=19, cex=1, cex.lab=1, cex.main=4, lwd=2)` & a histogram of the response variable: `r hist(df$O.ring, col="grey")`.
  - **Lastly, this model formulation possesses another shortcomings**:
    - The probabilities are linearly related to the predictors for all of their possible values.

- We select the binary logistic regression model because:
  - the response, O-ring failure is either binary (e.g. 0 or 1), binomial (e.g. a proportion from 0 to 1), or counts (e.g. 0,1,2,3,...,n) which can ultimately be expressed as a binary or binomial variable.
  -

**Part 5 (10 points)**

Interpret the main result of your final model in terms of both odds and probability of failure. Summarize the final result with respect to the question(s) being asked and key takeaways from the analysis.

Our final model model is $$logit\left(\cfrac{O.ring}{Number}\right) = 5.085 - 0.116 Temp$$


This represents the log odds of the fraction of orings that fail. To calculate the odds, we exponentiate the coefficients as below. 


```{r}
exp(model_b$coefficients)
```

Here we see that an increase in temperature changes the odds of the ratio of failures by 0.89 for each degree increase, which could also be interpreted as a unit decrease in temperature increases the odds of failures.

