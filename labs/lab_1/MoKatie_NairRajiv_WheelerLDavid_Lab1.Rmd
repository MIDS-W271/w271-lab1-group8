---
title: "W271 Group Lab 1"
geometry: margin=1in
output:
  pdf_document: null
  word_document: default
  toc: yes
  number_sections: yes
students: Katie Mo, Rajiv Nair, David Linnard Wheeler
subtitle: Due 4:00pm Pacific Time Monday June 1 2020
fontsize: 11pt
---

## Instructions (Please Read Carefully):

* 20 page limit (strict)

* Do not modify fontsize, margin or line_spacing settings

* One student from each group should submit the lab to their student github repo by the deadline; submission and revisions made after the deadline will not be graded.

* Answers should clearly explain your reasoning; do not simply 'output dump' the results of code without explanation 

* Submit two files:
    
    1. A pdf file that details your answers. Include all R code used to produce the answers. Do not suppress the codes in your pdf file
    
    2. The R markdown (Rmd) file used to produce the pdf file
  
    The assignment will not be graded unless **both** files are submitted
      
* Name your files to include all group members names. For example the students' names are Stan Cartman and Kenny Kyle, name your files as follows:

    * `StanCartman_KennyKyle_Lab1.Rmd`
    * `StanCartman_KennyKyle_Lab1.pdf`
            
* Although it sounds obvious, please write your name on page 1 of your pdf and Rmd files

* All answers should include a detailed narrative; make sure that your audience can easily follow the logic of your analysis. All steps used in modelling must be clearly shown and explained

* For statistical methods that we cover in this course, use the R libraries and functions that are covered in this course. If you use libraries and functions for statistical modeling that we have not covered, you must provide an explanation of why such libraries and functions are used and reference the library documentation. For data wrangling and data visualization, you are free to use other libraries, such as dplyr, ggplot2, etc

* For mathematical formulae, type them in your R markdown file. Do not e.g. write them on a piece of paper, snap a photo, and use the image file

* Incorrectly following submission instructions results in deduction of grades

* Students are expected to act with regard to UC Berkeley Academic Integrity.

\newpage

# Investigation of the 1989 Space Shuttle Challenger Accident 

Carefullly read the Dalal et al (1989) paper (Skip Section 5).

### Load libraries
```{r warning=FALSE, message=FALSE}
library(ggplot2) # for plotting
library(GGally) # for scatterplot matrices
library(gridExtra) # for arranging multiple plots together
library(car) # For Likelihood Ratio Tests on the fly
library(data.table) # to enable creation and coercion of data tables
library(stargazer) # to tabulate regression tables
library(skimr) # for basic EDA
library(mcprofile) # for profile likelihoods
library(dplyr) # for coercing dataframes and summary statistic
#library(boot) # framework for bootstrapping
library(scatterplot3d) # For 3d scatterplot
```

### Load data
```{r}
df = read.table("challenger.csv", header=T, sep=",")
```

# Part 1 (25 points)

Conduct a thorough EDA of the data set. This should include both graphical and tabular analysis as taught in this course. Output-dump (that is, graphs and tables that don't come with explanations) will result in a very low, if not zero, score. Since the report has a page-limit, you will have to be selective when choosing visuals to illustrate your key points, associated with a concise explanation of the visuals. This EDA should begin with an inspection of the given dataset; examination of anomalies, missing values, potential of top and/or bottom code etc.   

### EDA

The dataset used in the examination of probability of O-ring failure in Challenger's previous space shuttle launches by Dalal *et al.* (1989) included 23 rows and 5 variables. The Flight column is made up of unique identifiers representing the flight number of a launch. The Temp column contains information about the temperature (F) at launch, and ranges between 53 and 81 degrees for the 23 data points. The Pressure column contains information about the combustion pressure (psi) at launch, and takes on only 3 values among the 23 data points: 50 100, or 200. The O.ring column represents the number of primary field O-ring failures in a launch, which is our outcome of interest. For this dataset, the O.ring column contains only the values 0, 1, or 2. The Number column contains the total number of primary field O-rings, which is 6 for all launches (three each for the two booster rockets). 

```{r}
# Structure
str(df)
```

```{r}
# Summary
summary(df)
```

```{r}
# Unique values
lapply(df[c('Temp','Pressure','O.ring')], unique)
```

Each of the 5 variables were treated as `int` classes. To ensure that the data do not contain anomolous observations, we  inspected the unique values for each variables. The values for each variable seem reasonable according to their description and we do not observe any top coded or bottom coded values (aside from the `O.ring` values > 0 that NASA ignored). We also noted that there are no missing values in any of the variables.


### Univariate Analysis

The three variables of interest for modeling purposes in the paper by Dalal *et al.* 1989 were temperature, pressure, and number of O-ring failures. 

```{r, fig.width=4, fig.height=2.5, echo=F}
# Temperature distribution
plot_temp = ggplot(df, aes(x = Temp)) +
  geom_histogram(binwidth = 1, fill="olivedrab", colour="black", alpha=0.7) + 
  ggtitle("B") +
    scale_x_continuous(name = "Temperature (°F)")+ 
    scale_y_continuous(name = " ")+
  theme(plot.title = element_text(lineheight=1)) +
  theme_classic()+
   theme(
   axis.title.x = element_text(size = 12),
   axis.text.x = element_text(size = 12),
   axis.title.y = element_text(size = 12),
   axis.text.y = element_text(size = 12),
   plot.title = element_text(size = 12))
```

```{r, fig.width=4, fig.height=2.5, echo=F}
# Pressure distribution
plot_pressure = ggplot(df, aes(x = Pressure)) +
  geom_histogram(binwidth = 5, fill="olivedrab", colour="black", alpha=0.7) + 
  ggtitle("C") +
  scale_x_continuous(name = "Pressure")+
  scale_y_continuous(name = " ")+ 
  theme(plot.title = element_text(lineheight=1)) +
  theme_classic() +
   theme(
   axis.title.x = element_text(size = 12),
   axis.text.x = element_text(size = 12),
   axis.title.y = element_text(size = 12),
   axis.text.y = element_text(size = 12),
   plot.title = element_text(size = 12))
```

```{r, fig.width=4, fig.height=2.5, echo=F}
# O.ring distribution
plot_oring = ggplot(as.data.frame(table(df)),
       aes(x = as.factor(O.ring), y = Freq, colour=O.ring>0)) + 
    geom_bar(stat="identity", width=0.1, alpha =0.7, color = "black", fill= "olivedrab") +
    scale_x_discrete(name = "Number of O-ring failures")+
    scale_y_continuous(name = "Counts")+
   theme_classic() +
   theme(text = element_text(size = 12)) +
   labs(title=expression(~bold('Figure 1:')~'A'))+
   theme(legend.position = "top") +  
   theme(plot.title = element_text(hjust=0)) + 
   theme(
   axis.title.x = element_text(size = 12),
   axis.text.x = element_text(size = 12),
   axis.title.y = element_text(size = 12),
   axis.text.y = element_text(size = 12),
   plot.title = element_text(size = 12))
``` 

```{r, fig.width=8, fig.height=2.5, warning=F, echo=F}
# Figure 1
grid.arrange(plot_oring, plot_temp, plot_pressure, nrow=1)
```

In **Figure 1 (a)**, the response variable `O.ring` is positively skewed with the majority of O-ring failures at 0. We observe that there are no O-ring failures in 16 flights, one O-ring failure in 5 flights, and two O-ring failures in 2 flights. 

From the histogram in **Figure 1 (b)**, the temperature variable, `Temp`, is seen to peak at 70 degrees with 4 observations and tapered off on either side. Based on the plot, the distribution seems to most closely resemble a normal distribution.

`Pressure` is seen to only take on 3 unique values, with the most observations having a pressure of 200, as seen in **Figure 1 (c)**. Though it is difficult to say with such few unique values, the distribution seems to most closely resemble a bimodal distribution. Although one could argue that `Pressure` should be coerced into a factor because of the low ...


### Bivariate Analysis

```{r, fig.width=6, fig.height=3, echo=F}
# O-ring vs Temperature
plot_temp_o = ggplot(df, aes(x=Temp, y=as.factor(O.ring), colour=O.ring>0)) +
  geom_jitter(size = 3, alpha = 0.75, stroke=3, height=.05, width=.3) +
  scale_color_manual(labels = c("ignored by NASA","used by NASA"),
                                values = rev(c("black","olivedrab"))) +
  scale_x_continuous(name = "Temperature (°F)")+
  scale_y_discrete(name = "Number of O-ring failures") +
  geom_rug(alpha=2/4, size=3, position="jitter") +
  theme_classic() +
  theme(text = element_text(size = 12)) +
  labs(title=expression(~bold('Figure 2:')~'A'),
       color="Observations:")+
  theme(legend.position = "top") +  
  theme(plot.title = element_text(hjust=0)) + 
  theme(
  axis.title.x = element_text(size = 12),
  axis.text.x = element_text(size = 12),
  axis.title.y = element_text(size = 12),
  axis.text.y = element_text(size = 12),
  plot.title = element_text(size = 12)) 
```

```{r, fig.width=6, fig.height=3, echo=F}
# O-ring vs Pressure
plot_pressure_o = ggplot(df, aes(x=Pressure, y=as.factor(O.ring), colour=O.ring>0)) +
  geom_jitter(size=4, pch=20, alpha=3/4, stroke=3, position=position_jitter(w=0.2, h=0.1))+
  scale_color_manual(labels = c("ignored by NASA","used by NASA"),
                                values = rev(c("black","olivedrab"))) +
  scale_x_continuous(name = "Pressure")+
  scale_y_discrete(name =" ")+
  geom_rug(alpha=2/4, size=3, position="jitter") +
  theme_classic() +
  theme(text = element_text(size = 12)) +
  labs(title='B')+
  theme(legend.position = "none") +
  theme(plot.title = element_text(hjust=0)) + 
  theme(
  axis.title.x = element_text(size = 12),
  axis.text.x = element_text(size = 12),
  axis.title.y = element_text(size = 12),
  axis.text.y = element_text(size = 12),
  plot.title = element_text(size = 12))
```

```{r, fig.width=7, fig.height=4, echo=F, results='hide'} 
# O-ring vs Flight number
plot_number_o = ggplot(df, aes(x=Flight, y=as.factor(O.ring), colour=O.ring>0)) +
  geom_jitter(size=3, pch=20, alpha=3/4, stroke=3, position=position_jitter(w=0.2, h=0.1))+
  scale_color_manual(labels = c("ignored by NASA","used by NASA"),
                                values = rev(c("black","olivedrab"))) +
  scale_x_continuous(name = "Flight number")+
  scale_y_discrete(name = " ")+
  geom_rug(alpha=2/4, size=3, position="jitter") +
  theme_classic() +
  theme(text = element_text(size = 12)) +
  labs(title='C') +
  theme(legend.position = "none") +
  theme(plot.title = element_text(hjust=0)) + 
  theme(
  axis.title.x = element_text(size = 12),
  axis.text.x = element_text(size = 12),
  axis.title.y = element_text(size = 12),
  axis.text.y = element_text(size = 12),
  plot.title = element_text(size = 12))
```

```{r, fig.width=8, fig.height=3, echo=F, warning=F}
# Figure 2
grid.arrange(plot_temp_o, plot_pressure_o, plot_number_o, nrow=1)
```

When plotting the temperature against the number of O-ring failures, as shown in **Figure 2 (a)** the launches with no O-ring failures tend to occur on the right hand side of the graph, where temperature is higher. This is consistent with the negative correlation of `r round(cor(df$Temp, df$O.ring),3)`. The lowest temperature at which there were no recorded O-ring failures was observed to be 66 °F. Above this temperature, we have one launch with 1 O-ring failure and one launch with 2 O-ring failures. The launches below 66 °F all had at least 1 O-ring failure. The authors of the paper mentioned that launches with no O-ring failures were not included in the original thermal distress analysis prior to the Challenger accident, which are highlighted in green in the plot above. 

In **Figure 2 (b)**, there were the most number of recorded launches at 200 psi, and at this pressure, there was the most number of launches with at least 1 O-ring failure. At 200 psi, 4 out of the 15 launches (27%) had at least 1 O-ring failure. At 50 psi, 1 out of the 6 launches (17%) had at least 1 O-ring failure. There were no O-ring failures for launches at 100 psi, though there were only 2 recorded data points. The correlation between the two variables is slightly positive, at `r round(cor(df$O.ring, df$Pressure),3)`.

Based on the description in the paper, we do not anticipate there to be any meaningful relationship between temperature and pressure as they are both arbitrarily determined. Pressure is a condition set by the test procedure, which was first at 50 psi, then progressively updated to 100 psi and 200 psi. Temperature is merely a condition at the time of launch. The low correlation between the two variables, `r round(cor(df$Temp, df$Pressure),3)`, supports our understanding.

Lastly, **Figure 2 (c)** documents the relationship between the number of O-ring failures and flight number. If flight numbers were not assigned chronologically, then there is little to learn from this plot. However, if flight numbers are ordered chronologically then (i) note that the occurrence of failures *over time* appear nearly random- that is, there is no overt pattern- and (ii) the history of flight attempts is punctuated by episodes of failure.


### Multivariate Analysis

```{r}
# Figure 3
#3d scatter plot
#Colors
colors <- c("olivedrab", "orange", "red")
colors <- colors[as.numeric(df$O.ring)+1]
scatterplot3d(x=df$Temp, y=df$Pressure, z=df$O.ring,
              angle = 85,
              main= expression(~bold('Figure 3:')~' O-ring failure as a function of temperature and pressure'),
              xlab = "Temperature (°F)",
              ylab = "Pressure",
              zlab = "Number of O-ring failures",
              pch = 20, cex.symbols=3,
              color=colors,
              grid=TRUE, box=F)

#library(plotly)
# # Build 3d scatter plot
# fig <- plot_ly(df, x = ~ Temp, y = ~ Pressure, z = ~ O.ringFactor,
#                color = ~ O.ringFactor,
#                colors = c('olivedrab', 'orange2',"orangered2"))
# fig <- fig %>% add_markers()
# fig <- fig %>% layout(title=" <b>Figure 5</b>: O-ring failures as a function of temperature (°F) and pressure",
#   scene = list(xaxis = list(title = 'Temperature (°F)'),
#                      yaxis = list(title = 'Pressure'),
#                      zaxis = list(title = 'Number of O-ring failures')))
# 
# fig
```

In order to understand the multivariate relationship between the number of O-ring failures, temperature, and pressure, we created a 3-dimensional scatterplot (**Figure 3**). The number of O-ring failures is expressed as a function of both temperature and pressure. Green points represent those temperature and pressure conditions where 0 O-rings failed- these were the data ignored by NASA. Similarly, orange and red points represent those conditions under which 1 or 2 O-rings failed, respectively. From **Figure 3** we can see that all but 1 of the cases where at least one or more O-rings failed the pressure was 200 and the temperature was less than `r max(df$Temp[df$O.ring>0])`. Lastly, **please** look at this plot in `plotlly` with the above code that is currently commented out- `scatterplot3d` really fails to capture the integrity of the relationships between these variables.

# Part 2 (20 points)

Answer the following from Question 4 of Bilder and Loughin Section 2.4 Exercises (page 129):

(a) The authors use logistic regression to estimate the probability an O-ring will fail. In order to use this model, the authors needed to assume that each O-ring is independent for each launch. Discuss why this assumption is necessary and the potential problems with it. Note that a subsequent analysis helped to alleviate the authors’ concerns about independence.

The authors state that the binomial but not the binary model suffer from the assumption that each O-ring failure is independent. More specifically, the binomial model assumes that each of the six 0-rings fail independently at each temperature, $t$, and pressure, $s$. This assumption is likely to be violated since the failure of 1 or more of the O-rings influences the failure of the remaining O-rings. Moreover, the binary logistic regression model does not suffer from this assumption since each *trial* is a flight and we assume the flights are independent. 

The independence assumption is necessary for the binomial model because it allows us to use the binomial probability mass function. Moreover, the assumption is likely violated since the failure of 1 0-ring might expose other 0-rings to damaging conditions and or might exert stress on the remaining 0-rings.

The authors subsequently used a binary logistic regression model by introducing a binary flag for failure. One or more failures were simply marked as a failure, thereby removing the independence assumption.


(b) Estimate the logistic regression model using the explanatory variables in a linear form.

```{r, warning=F}
df["Fail"] <- ifelse(df$O.ring>=1, 1, 0)
# Estimate binomial logistic regression model
model_binomial <- glm(O.ring/Number ~ Temp + Pressure,
               family = binomial(link = logit),
               weights = Number, data = df)
# Estimate binary logistic regression model
model_binary <- glm(Fail ~ Temp + Pressure,
               family = binomial(link = logit),
               data = df)
# Summary of coefficients, standard errors and p-values
stargazer(model_binomial, model_binary, type="text")
```

Here we estimate the bimonial and binary regression models separately. For the binary model, we introduce a new flag "Fail" that is set to 1 if at least one failure, 0 otherwise. This effectively eliminates the independence assumption. From the above model summary, we see that Temperature is the only significant variable at the 0.05 level. By exponentiating the coefficients and taking inverse, we can interpret the effect of temperature on odds of failure.

**Binomial Model**
$$ logit\left(\hat{\pi}\right)= 2.520 - 0.098\text{Temp} + 0.008\text{Pressure} $$
```{r}
# Inverse odds ratio for binomial model
1/exp(summary(model_binomial)$coefficients)
```
For the Logistic regression binomial model with `Temperature` and `Pressure` included, we see that a unit decrease in temperature changes the odds of failure by 1.1 times

**Binary Model** 
$$ logit\left(\hat{\pi}\right)= 13.292 - 0.229\text{Temp} + 0.010\text{Pressure} $$ 
```{r}
# Inverse odds ratio for binary model
1/exp(summary(model_binary)$coefficients)
```
For the Logistic regression binary model with `Temperature` and `Pressure` included, we see that a unit decrease in temperature changes the odds of failure by 1.25 times.

```{r, echo=F}
# Figure 3
b <- model_binomial$coefficients
# Binomial
curve(exp(b[1]+b[2]*x + b[3]*200)/(1+exp(b[1]+b[2]*x + b[3]*200)),
      from=30, to=100,
      xlab="Temperature", ylab=expression(pi),
      col="blue", lwd=2, ylim=c(0,1))
text(x=40, y=0.7, label="Binomial", col = "Blue")
# Binary
b <- model_binary$coefficients
curve(exp(b[1]+b[2]*x+ b[3]*200)/(1+exp(b[1]+b[2]*x+ b[3]*200)),
      from=30, to=100, xlab="Temperature",
      ylab=expression(1-pi), add=TRUE, col="black", lwd=2)
text(x=65, y=0.7, label="Binary", col = "Black")
# Title
title(expression(bold("Figure 4:")~"Binomial vs binary logistic regression (Pressure = 200)"), adj=0)
```

(c) Perform LRTs to judge the importance of the explanatory variables in the model.

Below we test: $H_0: \beta = 0$ vs $H_A:\beta \neq 0$, for both `Temp` and `Pressure`:

```{r}
Anova(model_binomial, test='LR')
```

As discussed below `Temp` but not `Pressure` is important for the model.

(d) The authors chose to remove Pressure from the model based on the LRTs. Based on your results, discuss why you think this was done. Are there any potential problems with removing this variable?

- `Pressure` is not an important explanatory variable. 
- Above, we tested $H_0: \beta = 0$ vs $H_A:\beta \neq 0$, for `Pressure`.The $-2log(\Lambda)$ = `r Anova(model_binomial, test='LR')[2,1]` and the *p*-value of $P\left(\chi^2_1 > \right.$ `r Anova(model_binomial, test='LR')[2,1]`) = `r Anova(model_binomial, test='LR')[2,3]` > $\alpha$ 0.05. 
- Thus, `Pressure` was probably removed in response to it lack of importance detected by the likelihood ratio test & because it does not contribute much systematic variation to the model (i.e `Pressure` can only take values `r unique(df$Pressure)`). 
- Although it was removed, it *could* be an important covariate to include in the model. For example, recall from **Figure 3** that (i) both instances where 2 `O.ring`s failed occured when `Pressure` was high = $200$ and (ii) `r length(df$Pressure[df$O.ring == 1 & df$Pressure == 200])` of `r length(df$Pressure[df$O.ring == 1])` instances where 1 `O.ring` failed occured when `Pressure` was high = $200$. Similarly, the Dalal *et al* 1989 report (i) very weak evidence of a pressure effect and (ii) overlapping confidence intervals for the expected number of incidents when pressure was 50 and 200. Together these bits of evidence support the hypothesis that `Pressure` could contribute to the mechanism by which `O.rings` fail.
- Several problems could arise by the omission of `Pressure` from the model. These problems are both mechanistic and statistical. The mechanistic problem is outline in Dalal *et al* 1989. `O.ring` function and failure are highly influenced by `Pressure`. Thus, while a simple model without `Pressure` might be *sufficient* for ones purposes, a model with `Pressure` and, if more data existed, an interaction term between `Temp` and `Pressure` might help guide interpretation. The statistical problem(s) of removing `Pressure` could be numerous. For one, if `Pressure` is *correlated* with both the explanatory variable `Temp` and the outcome `O.ring`, removal of `Pressure` could introduce omitted variable bias.

# Part 3 (35 points)

Answer the following from Question 5 of Bilder and Loughin Section 2.4 Exercises (page 129-130):

Continuing Exercise 4, consider the simplified model $logit(\pi) = \beta_0 +  \beta_1 Temp$, where $\pi$ is the probability of an O-ring failure. Complete the following:

(a) Estimate the model.

```{r warning=FALSE}
# Estimate logistic regression model
model_b <- glm(O.ring/Number ~ Temp,
               family = binomial(link = logit),
               weights = Number,
               data = df)
# Summary of coefficients, standard errors and p-values
stargazer(model_b,  type="text")
```

$$logit\left(\hat{\pi}\right) = 5.085 - 0.116 Temp$$
(b) Construct two plots: (1) $\pi$ vs. Temp and (2) Expected number of failures vs. Temp. Use a temperature range of 31° to 81° on the x-axis even though the minimum temperature in the data set was 53°.

Please see **Figure 5** and below for discussion on (i) $\pi$ vs `Temp` without confidence intervals (**Figure 5a**), (ii) $\pi$ vs `Temp` with confidence intervals (**Figure 5b**), and the expected number of failures vs `Temp`(**Figure 5c**).

(c) Include the 95% Wald confidence interval bands for $\pi$ on the plot. Why are the bands much wider for lower temperatures than for higher temperatures?

```{r}
# Create an array for temperature between 31 and 81 degrees
t <- seq(31,81,1)
alpha=0.05
model_predict <- predict(object=model_b, newdata=data.frame(Temp=t), type='link', se=T)
CI_lower_linear <- model_predict$fit + qnorm(p=alpha/2)*model_predict$se.fit
CI_lower_pi <- exp(CI_lower_linear)/(1+exp(CI_lower_linear))
CI_higher_linear <- model_predict$fit + qnorm(p=1-alpha/2)*model_predict$se.fit
CI_higher_pi <- exp(CI_higher_linear)/(1+exp(CI_higher_linear))
```

```{r, echo=F}
# Figure 4
# Plot dimensions
par(mfrow=c(1,3))
##### O.ring failure vs temp
plot(df$Temp, df$O.ring/df$Number,
     xlab=" " ,
     ylab=expression(pi),
     pch=20, cex=3,
     col="black",
     xlim=c(31,81),
     ylim=c(0,1))
# Title
title(expression(bold("Figure 5:")~"A"), adj=0)
# Betas
b0 <- model_b$coefficients[1]
b1 <- model_b$coefficients[2]
# Curve
curve(expr = exp(b0+b1*x)/(1+exp(b0+b1*x)), add=T)

##### pi with CI vs temp
plot(df$Temp, df$O.ring/df$Number,
     xlab= "Temperature (°F)",
     ylab=expression(pi),
     pch=20, cex=3,
     xlim=c(31,81),
     ylim=c(0,1))
# Title
title(expression("B"), adj=0)
# Betas
b0 <- model_b$coefficients[1]
b1 <- model_b$coefficients[2]
curve(expr = exp(b0+b1*x)/(1+exp(b0+b1*x)), add=T)
lines(t, CI_lower_pi, lty = 'dashed')
lines(t, CI_higher_pi, lty = 'dashed')

##### Expected number of failures vs temp
plot(df$Temp, df$O.ring,
     xlab=' ',
     ylab="Expected number of O-ring failures",
     pch=20, cex=3,
     col="black",
     xlim=c(31,81),
     ylim=c(0,6))
# Title
title(expression("C"), adj=0)
## Binomial betas
# Betas
b0 <- model_b$coefficients[1]
b1 <- model_b$coefficients[2]
# Curve
curve(expr = exp(b0+b1*x)/(1+exp(b0+b1*x))*6, add=T)
```

- The confidence band is wider when temperatures are lower than`r min(df$Temp)` because there are no observations below `r min(df$Temp)`.

(d) The temperature was 31° at launch for the Challenger in 1986. Estimate the probability of an O-ring failure using this temperature, and compute a corresponding confidence interval. Discuss what assumptions need to be made in order to apply the inference procedures.

- The probability of O-ring failure when temperature is 31°:
```{r}
# Data to extrapolate
predict.data <- data.frame(Temp = 31)

# Predict surface/link
predict.linear <- predict(object = model_b,
                          newdata = predict.data,
                          type = "link")

# Predict response 
predict.pi <- predict(object = model_b, 
                      newdata = predict.data,
                      type = "response")

# Confidence interval for 31deg was computed above in part (c)
data.frame(estimate=predict.pi, lower=CI_lower_pi[[1]], upper=CI_higher_pi[[1]])
```
- Thus, the probability of O-ring failure at 31° is `r predict.pi` with a very wide confidence interval of `r c(CI_lower_pi[[1]], CI_higher_pi[[1]])`.
- The assumptions needed to apply the inference procedures are:
  - We are assuming a linear relationship between the log odds of failure of an O-ring and Temperature.
  - Since we are using the binary logistic model, each flight/*trial* is independent of other flights.
 
(e) Rather than using Wald or profile LR intervals for the probability of failure, Dalal *et al.* (1989) use a parametric bootstrap to compute intervals. Their process was to (1) simulate a large number of data sets (n = 23 for each) from the estimated model of  Temp; (2) estimate new models for each data set, say and (3) compute  at a specific temperature of interest. The authors used the 0.05 and 0.95 observed quantiles from the  simulated distribution as their 90% confidence interval limits. Using the parametric bootstrap, compute 90% confidence intervals separately at temperatures of 31° and 72°.27. At the end, the CI will be the 0.05 and 0.95 quantile of the accumumated results.

For our parametric bootstrap procedure, we performed the following steps:

1. Using our binomial model with temperature as the explanatory model, we calculate $Z$, the linear outcome, using $Z = \beta_0+\beta_{Temp}*Temp$, which results in a vector of size 23.

2. Next, we calculate $\hat\pi = \cfrac{e^Z}{ 1+e^Z}$, which remains a vector of size 23.

3. The calculated $\hat\pi$ is saved to the dataframe as `O.ring.pi`. 

4. The original dataset is resampled with replacement to create a new dataset $d$ of size 23.

5. A vector of size 23 of binomials 0-6 is generated as $O.ring2$, represent new outcomes, using the built in rbinom function by passing in the associated calculated $\hat\pi$ values.

6. A new binomial logistic regression model is fitted with the resampled dataset $d$ and outcomes $O.ring2$.

7. The predictions for 31 and 72 degrees are found and the estimated probabilities are saved to $results$.

8. Steps 4-7 are repeated and performed for a total of 1000 times.

9. Finally, the 5% and 95% quantiles from the predictions for 31 and 72 degrees accumulated in $results$ are reported as the 90% confidence intervals.


```{r warning=FALSE}
# Set a seed
set.seed(1)

# Use pi estimated from the model
z = model_b$coefficients["(Intercept)"] + model_b$coefficients["Temp"] * df$Temp
pi = exp(z)/(1+exp(z))

# Save the pi array to the dataframe
df$O.ring.pi <- pi

# Dataframe to populate with results
results <- data.frame(pred.31 = numeric(), pred.72 = numeric())

for (s in 1:1000){
  
  I.sample <- sample(x = 1:nrow(df),
                     size = 23,
                     replace = T)
  # Populate d with samples
  d <- df[I.sample,]  
  
  # Simulate outcomes using rbinom
  O.ring2 <- rbinom(n=23, # sample size
                size=6, # number of trials
                prob=d$O.ring.pi) # probability
  d <- data.frame(d, O.ring2)
  
  # Estimate model with rbinom bootstrap outcomes
  mod <- glm(O.ring2/Number ~ Temp,
               family = binomial(link = logit),
               weights = Number,
               data = d)
  
  # Estimate confidence interval for temp = 31
  temp.31.data <- data.frame(Temp=31)
  temp.31 <- predict(object = mod, newdata = temp.31.data,
                      type = "response")
  # Estimate confidence interval for temp = 72
  temp.72.data <- data.frame(Temp=72)
  temp.72 <- predict(object = mod, newdata = temp.72.data,
                      type = "response")
  results <- results %>% add_row(pred.31 = temp.31, pred.72 = temp.72)  
}

paste("For 31 degrees CI =")
quantile(results[,1], probs=c(0.05, 0.95))
paste("For 72 degrees CI =")
quantile(results[,2], probs=c(0.05, 0.95))
```

```{r, echo=F}
# Plot histograms of confidence intervals
par(mfrow=c(2,1))
# Histogram for temp = 31
hist(results[,1],
     breaks=50,
     col="lightskyblue3",
     xlab=NULL,
     xlim=c(0,1),ylim=c(0,900),
     main="A", adj=1)
# Title
title(expression(bold("Figure 6:")~"Probability of failure when temperature:"), adj=0)
# Histogram for temp = 72
hist(results[,2],
     breaks=20,
     col="orangered",
     xlab=NULL,
     xlim=c(0,1),ylim=c(0,900),
     main="B", adj=1)
```
From the parametric bootstrapping above we learned (i) the range of estimates for a `Temp` = 31(°F) was very wide (**Figure 6a**), ranging from `r min(results[,1])` to `r max(results[,1])`, (ii) most of the estimates were were near 1 and thereby assigned a high probability of failure and (iii) the range of estimates `Temp` = 72(°F) was relatively narrow, ranging from `r min(results[,2])` to `r max(results[,2])` (**Figure 6b**).

(f) Determine if a quadratic term is needed in the model for the temperature.

- Add quadratic temperature term
```{r warning=FALSE}
# Estimate logistic regression model
model_c <- glm(O.ring/Number ~ Temp + I(Temp^2),
               family = binomial(link = logit),
               weights = Number,
               data = df)
# Summary of coefficients, standard errors and p-values
stargazer(model_c, type="text")
```

$$ logit\left(\hat{\pi}\right)=22.126 -0.651Temp + 0.004Temp^2 $$ 

- Conduct LRT:
```{r}
# Compare models with LRT
anova(model_b, model_c, test="Chisq")
```

- No, there is no need for a quadratic term. 
- We tested $H_0: \beta_2{\text{temp}^2} = 0$ vs $H_A: \beta_2{\text{temp}^2} \neq 0$.
- For `temp^2`, $-2log(\Lambda)$ = `r anova(model_b, model_c, test="Chisq")[2,4]` and the *p*-value of $P\left(\chi^2_1 > \right.$ `r anova(model_b, model_c, test="Chisq")[2,4]`) = `r anova(model_b, model_c, test="Chisq")[2,5]` > $\alpha = 0.05$. Thus, `temp^2` is not important, when we hold `temp` constant.

# Part 4 (10 points)

With the same set of explanatory variables in your final model, estimate a linear regression model. Explain the model results; conduct model diagnostic; and assess the validity of the model assumptions.  Would you use the linear regression model or binary logistic regression in this case?  Explain why.

- Estimation of linear regression model:
```{r warning=FALSE}
# Estimate model
model_d = lm(formula = O.ring/Number ~ Temp, data = df, weights = Number)
# Summary
stargazer(model_d, type="text")
```
$$ \cfrac{O.ring}{Number} = 0.616 - 0.008Temp$$

- Explanation of the model results:

- Model diagnostics:
```{r, echo=F}
# Plot histograms of confidence intervals
par(mfrow=c(1,2))
# Residuals vs fitted values
plot(model_d$fitted.values, model_d$residuals,
     pch=20, cex=2, main=NULL,
     xlab="Fitted values",
     ylab="Residuals")
# Title
title(expression(bold("Figure 7:")~"A"), adj=0)
# Quantile-quantile plot
qqnorm(df$O.ring,
     pch=20, cex=2, main=NULL,
     xlab="Sample quantiles",
     ylab="Theoretical quantiles")
# Add line
qqline(df$O.ring,
       col="red", lwd=2)
# Title
title(expression("B"), adj=0)
```

- Assess the validity of the model assumptions: 
  - **Assumption MLR.1 (Linear in Parameters):**
    - The model is linear in its parameters.The $\hat{\beta} s$ are expressed as additive/linear functions of the response: O-ring failure.
  - **Assumption MLR.2 (Random Sampling):**
    - It seems doubtful that these O-rings were sampled randomly, or even *sampled* at all- **all** O-rings were likely collected from `r max(df$Flight)` flights. Further survival bias could be introduced by the inability of NASA to collect all O-rings from flights that malfunctioned.
  - **Assumption MLR.3 (No Perfect Collinearity):**
    - ince there is only one explanatory variable, it is not perfectly collinear with any other explanatory variable.
  - **Assumption MLR.4 (Zero Conditional Mean):**
    - The zero-conditional mean assumption,$E(\mu|x_1, x_2, x_3,...,x_k)$, is not satisfied, sensu stricto. The residual versus fitted values plot (**Figure 7a**) documents the extent to which this model violates this assumption. If the zero mean assumption were satisfied, then we should expect to see a horizontal red line, centered at zero. In contrast we see that our estimates of $\hat{\mu}$, the residuals, deviate from this expectation dramatically. 
  - **Assumption MLR.5 (Homoskedasticity):**
    - Homoskedasticity does not appear to be satisfied. Evidence for heteroskedasticity is again provided by the residual versus fitted values plot (**Figure 7a**). If variances were homoskedastic we should see uniform horizontal bands/scatters of points across these plots. However, note that we see non-constant variability in the residuals across the range of fitted values.
  - **Assumption MLR.6 (Normality):**
    - The assumption of normality is not satisfied. Evidence of this assumption is presented in the qq-plot  (**Figure 7b**).
  - **Lastly, this model formulation possesses another shortcomings**:
    - The probabilities are linearly related to the predictors for all of their possible values.

- Explain why:
- We select the binary logistic regression model because:
  - The problem is clearly asking a success/failure question. The response, O-ring failure is either binary (e.g. 0 or 1), binomial (e.g. a proportion from 0 to 1), or counts (e.g. 0,1,2,3,...,n) which can ultimately be expressed as a binary or binomial variable.
  - The MLR assumptions as seen above does not hold and therefore the linear regression model is not suitable for this problem.

# Part 5 (10 points)

Interpret the main result of your final model in terms of both odds and probability of failure. Summarize the final result with respect to the question(s) being asked and key takeaways from the analysis.

Our final model model is the binomial logistic regression model $$logit\left(\hat\pi\right) = 5.085 - 0.116 Temp$$

This represents the log odds of the fraction of O-rings that fail. To calculate the odds, we exponentiate the coefficients as below. 

```{r}
exp(model_b$coefficients)

1/exp(model_b$coefficients)
```

Here we see that a unit decrease in temperature increases the odds of failure increases by 1.12 times.

In terms of probability of failure, as is seen in the chart of probability of failure vs temperature in part 3c, we clearly see an increase in the probability of failure with lower temperature. For temperature going from 50F, 40F to 30F, we see corresponding probability of failure $\pi$ values of  0.33, 0.61 to 0.83, which clearly shows that lower temperatures lead to a higher probability of failure.

In summary, we explored the relationship to pressure vs failure and temperature vs failure. We note a higher incidence of failures at lower temperatures, wheras no strong evidence of a relationship to pressure was identified. 

- EDA
- Potential importance of pressure
- Reasons for binary vs binomial
- Interpretation of odds ratios & pi at extreme temperaturs
- Challenges introduced by small data set (at lower temperatures)